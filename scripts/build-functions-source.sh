#!/usr/bin/env bash
# Build and upload Cloud Functions source archive for Terraform deployment.
#
# Usage: ./scripts/build-functions-source.sh [env]
#   env: dev (default), staging, prod
#
# This script:
# 1. Builds the shared package (dependency)
# 2. Builds the functions package
# 3. Creates an env-isolated staging directory with shared bundled via file: protocol
# 4. Creates a ZIP archive of the built output
# 5. Uploads to the GCS functions source bucket
# 6. Updates terraform.auto.tfvars with the source object path

set -euo pipefail

ENV="${1:-dev}"
PROJECT_ID="equiduty-${ENV}"
BUCKET="${PROJECT_ID}-functions-source"
ROOT_DIR="$(cd "$(dirname "$0")/.." && pwd)"
FUNCTIONS_DIR="${ROOT_DIR}/packages/functions"
SHARED_DIR="${ROOT_DIR}/packages/shared"
TF_DIR="${ROOT_DIR}/terraform/environments/${ENV}"
BUILD_DIR="${ROOT_DIR}/.build"
STAGE_DIR="${BUILD_DIR}/functions-${ENV}"
TIMESTAMP=$(date +%Y%m%d-%H%M%S)
ARCHIVE_NAME="functions-${TIMESTAMP}.zip"
ARCHIVE_PATH="${BUILD_DIR}/${ARCHIVE_NAME}"

cleanup() {
  rm -rf "${STAGE_DIR}"
}
trap cleanup EXIT

echo "=== Building Functions Source for ${ENV} ==="

# Step 1: Build shared package
echo "Building shared package..."
cd "${SHARED_DIR}"
npm run build

# Step 2: Build functions package
echo "Building functions package..."
cd "${FUNCTIONS_DIR}"
npm run build

# Step 3: Create env-isolated staging directory
echo "Preparing staging directory: ${STAGE_DIR}"
rm -rf "${STAGE_DIR}"
mkdir -p "${STAGE_DIR}"

# Copy built functions output
cp -r "${FUNCTIONS_DIR}/lib" "${STAGE_DIR}/lib"

# Copy shared package (dist + package.json) for file: protocol resolution
mkdir -p "${STAGE_DIR}/shared"
cp -r "${SHARED_DIR}/dist" "${STAGE_DIR}/shared/"
cp "${SHARED_DIR}/package.json" "${STAGE_DIR}/shared/"

# Use package.deploy.json (has "@equiduty/shared": "file:./shared")
cp "${FUNCTIONS_DIR}/package.deploy.json" "${STAGE_DIR}/package.json"

# Copy lockfile if available
cp "${FUNCTIONS_DIR}/package-lock.json" "${STAGE_DIR}/" 2>/dev/null || true

# Step 4: Install production dependencies in staging dir
echo "Installing production dependencies..."
cd "${STAGE_DIR}"
npm install --omit=dev --ignore-scripts 2>/dev/null || npm install --production --ignore-scripts

# Step 5: Create ZIP archive
echo "Creating source archive..."
mkdir -p "${BUILD_DIR}"
cd "${STAGE_DIR}"
zip -r "${ARCHIVE_PATH}" . -x "*.ts" "*.map" > /dev/null

echo "Archive created: ${ARCHIVE_PATH}"

# Step 6: Upload to GCS
echo "Uploading to gs://${BUCKET}/${ARCHIVE_NAME}..."
gsutil cp "${ARCHIVE_PATH}" "gs://${BUCKET}/${ARCHIVE_NAME}"

# Clean up local archive
rm -f "${ARCHIVE_PATH}"
echo "Upload complete."

# Step 7: Update tfvars with source reference
echo "Updating Terraform variables..."
TFVARS_AUTO="${TF_DIR}/terraform.auto.tfvars"

# Create or update the auto tfvars file (Terraform auto-loads *.auto.tfvars)
if [ -f "${TFVARS_AUTO}" ]; then
  # Update existing values
  if grep -q "functions_source_bucket" "${TFVARS_AUTO}"; then
    sed -i.bak "s|functions_source_bucket.*=.*|functions_source_bucket = \"${BUCKET}\"|" "${TFVARS_AUTO}"
    rm -f "${TFVARS_AUTO}.bak"
  else
    echo "functions_source_bucket = \"${BUCKET}\"" >> "${TFVARS_AUTO}"
  fi

  if grep -q "functions_source_object" "${TFVARS_AUTO}"; then
    sed -i.bak "s|functions_source_object.*=.*|functions_source_object = \"${ARCHIVE_NAME}\"|" "${TFVARS_AUTO}"
    rm -f "${TFVARS_AUTO}.bak"
  else
    echo "functions_source_object = \"${ARCHIVE_NAME}\"" >> "${TFVARS_AUTO}"
  fi
else
  cat > "${TFVARS_AUTO}" << EOF
# Auto-generated by build-functions-source.sh
# Last updated: $(date -u +%Y-%m-%dT%H:%M:%SZ)
functions_source_bucket = "${BUCKET}"
functions_source_object = "${ARCHIVE_NAME}"
EOF
fi

echo ""
echo "=== Build Complete ==="
echo "  Environment: ${ENV}"
echo "  Bucket:      gs://${BUCKET}"
echo "  Object:      ${ARCHIVE_NAME}"
echo "  Tfvars:      ${TFVARS_AUTO}"
echo ""
echo "Next: Run 'task deploy:functions ENV=${ENV}' or 'cd ${TF_DIR} && terraform apply -target=module.cloud_functions'"
